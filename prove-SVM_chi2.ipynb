{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/utente_locale/anaconda3/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "SCRIPT PER PROVARE configurazioni SVM\n",
    "'''\n",
    "\n",
    "import time\n",
    "t_orig = time.process_time()\n",
    "last_time = 0\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import codecs\n",
    "import csv\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from time import time\n",
    "from dill import dill\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn import cross_validation, svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.tree.tree import DecisionTreeClassifier\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# modulo per confrontare il risultato delle regex\n",
    "# richiede entrambi i db già etichettati\n",
    "import tortellino_parmigiano \n",
    "import crea_db\n",
    "\n",
    "\n",
    "import evaluation_report # stampa confusion_matrix e altro\n",
    "from text_tokenizer import get_tokenizer \n",
    "from nltk.stem.snowball import ItalianStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ita_stopwords = set(stopwords.words('italian'))\n",
    "ita_stemmer = ItalianStemmer()\n",
    "ita_stemming = ita_stemmer.stem\n",
    "def get_tokenizer1(doc): #(word_ngrams=None, char_ngrams=None, stopwords=None, additional_extractors=None):\n",
    "    # rimuove le stopwords\n",
    "    doc = filter(lambda x: x not in ita_stopwords, word_tokenize(doc))\n",
    "    # stemma le parole\n",
    "    doc = [ita_stemmer.stem(a) for a in doc]\n",
    "    return doc\n",
    "\n",
    "def get_tokenizer(doc): #(word_ngrams=None, char_ngrams=None, stopwords=None, additional_extractors=None):\n",
    "    doc = filter(lambda x: x not in ita_stopwords, word_tokenize(doc))\n",
    "    #doc = map(ita_stemming, doc)\n",
    "    #doc = [ita_stemmer.stem(a) for a in doc]\n",
    "    temp = ''\n",
    "    for par in doc:\n",
    "        temp = '{} {}'.format(temp, ita_stemming(par))\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sqlite e csv presenti!\n",
      "Test sqlite e csv presenti!\n",
      "Trovato salvataggio training set\n",
      "Salvataggio caricato\n"
     ]
    }
   ],
   "source": [
    "DATA_FOLDER = \"/home/backup/dati_e_csv/\" # DATI\n",
    "# ci assicuriamo siano presenti i DB\n",
    "training_db_name = crea_db.build_training(OUTPUT=DATA_FOLDER)\n",
    "test_db_name = crea_db.build_test(OUTPUT=DATA_FOLDER) ################################\n",
    "PATH = '/home/backup/dati_e_csv/training_stemmed.pickle'\n",
    "    \n",
    "if os.path.exists(PATH):\n",
    "    print('Trovato salvataggio training set')\n",
    "#     with open('/home/backup/dati_e_csv/training_stemmed.pickle', 'rb') as file:\n",
    "#         salvataggio = pickle.load(file)\n",
    "#     texts = salvataggio.features\n",
    "#     texts_stemmed = salvataggio.stemmed\n",
    "#     Y = salvataggio.labels\n",
    "    salvataggio = pd.read_pickle(PATH)\n",
    "    etichette = salvataggio[\n",
    "        salvataggio.columns.difference(['testo', 'testo_stemmato'])].columns.values.tolist()\n",
    "    print('Salvataggio caricato')\n",
    "else:\n",
    "    print('Creo il training set')\n",
    "    \n",
    "    texts = list()\n",
    "    labels = list()\n",
    "    names= []\n",
    "\n",
    "    # load training dataset # possiamo usare sqlite\n",
    "    with codecs.open(os.path.join(DATA_FOLDER, training_db_name), 'r', errors='ignore') as file:\n",
    "        reader = csv.reader(file, delimiter=';')\n",
    "        etichette = next(reader)\n",
    "        for row in reader:\n",
    "            # load documents\n",
    "            #print('Nome {}, etichette {}, conversazione{}'.format(row[10], row[:10], row[11]+row[12]))\n",
    "            #raise SystemExit\n",
    "            unito = (row[11]+row[12]).replace('#O:', '').replace('#C:', '') #concatena il testo cliente con operatore\n",
    "            texts.append(unito)\n",
    "            # load row name\n",
    "            names.append(row[10])\n",
    "            # load labels\n",
    "            labels.append(list(map(lambda x: int(x), row[:10])))\n",
    "\n",
    "    print('dimensioni:',len(texts), len(labels))\n",
    "    # tokenization function # possiamo usare anche solo un canale cliente/operatore\n",
    "    print('Inizio lo stemming')\n",
    "    etichette = etichette[:10]\n",
    "    salvataggio = pd.DataFrame()\n",
    "    salvataggio['testo'] = texts\n",
    "    salvataggio[etichette] = pd.DataFrame(Y)\n",
    "    salvataggio['testo_stemmato'] = pd.Series(list(map(get_tokenizer, texts)))\n",
    "    print('Training set creato e salvato')\n",
    "    \n",
    "    # save DataFrame\n",
    "    salvataggio.to_pickle(PATH)\n",
    "    \n",
    "#     salvataggio = CestinoTraining(texts, Y, etichette)\n",
    "#     texts_stemmed = salvataggio.stemmed\n",
    "#     Y = salvataggio.labels\n",
    "#     with open('/home/backup/dati_e_csv/training_stemmed.pickle', 'wb') as file:\n",
    "#         pickle.dump(salvataggio, file, protocol=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['attivazione',\n",
       " 'canone rai',\n",
       " 'cessazione',\n",
       " 'contratto',\n",
       " 'credito',\n",
       " 'domiciliazione',\n",
       " 'fatturazione',\n",
       " 'gr-invito a chiamare - focus',\n",
       " 'non di competenza',\n",
       " 'richiamate']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salvataggio[salvataggio.columns.difference(['testo', 'testo_stemmato'])].columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>testo</th>\n",
       "      <th>attivazione</th>\n",
       "      <th>canone rai</th>\n",
       "      <th>cessazione</th>\n",
       "      <th>contratto</th>\n",
       "      <th>credito</th>\n",
       "      <th>domiciliazione</th>\n",
       "      <th>fatturazione</th>\n",
       "      <th>gr-invito a chiamare - focus</th>\n",
       "      <th>non di competenza</th>\n",
       "      <th>richiamate</th>\n",
       "      <th>testo_stemmato</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sono geloso macchina perché  che questo  mi ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>gelos macchin dann numer nient quand prossim ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sì  hanno hanno inviato documentazioni fonte...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>sì invi document font signor numer client ved...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>io la sento malissimo  sì ma la bolletta  il...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>sent malissim sì bollett numer civic dic codi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spetti  si senta  lei si esprime  ma il moti...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>spett sent esprim mot collett dic volt chiam ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sì  per attivare che il contratto segnalasse...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>sì attiv contratt segnal ' abit richiest può ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               testo  attivazione  canone rai  \\\n",
       "0    sono geloso macchina perché  che questo  mi ...            0           0   \n",
       "1    sì  hanno hanno inviato documentazioni fonte...            0           0   \n",
       "2    io la sento malissimo  sì ma la bolletta  il...            0           0   \n",
       "3    spetti  si senta  lei si esprime  ma il moti...            0           0   \n",
       "4    sì  per attivare che il contratto segnalasse...            0           0   \n",
       "\n",
       "   cessazione  contratto  credito  domiciliazione  fatturazione  \\\n",
       "0           0          0        1               0             0   \n",
       "1           0          0        0               0             0   \n",
       "2           0          0        0               0             0   \n",
       "3           0          0        1               0             0   \n",
       "4           0          0        0               0             0   \n",
       "\n",
       "   gr-invito a chiamare - focus  non di competenza  richiamate  \\\n",
       "0                             1                  0           0   \n",
       "1                             1                  0           0   \n",
       "2                             0                  0           0   \n",
       "3                             1                  0           1   \n",
       "4                             1                  0           0   \n",
       "\n",
       "                                      testo_stemmato  \n",
       "0   gelos macchin dann numer nient quand prossim ...  \n",
       "1   sì invi document font signor numer client ved...  \n",
       "2   sent malissim sì bollett numer civic dic codi...  \n",
       "3   spett sent esprim mot collett dic volt chiam ...  \n",
       "4   sì attiv contratt segnal ' abit richiest può ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salvataggio.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load create the model:  5.437809368\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "print('Time to load create the model: ', time.process_time()-last_time)\n",
    "last_time = time.process_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# some features selection\n",
    "# stopwords, tf-idf\n",
    "X_traing = salvataggio['testo_stemmato']\n",
    "count_vect = CountVectorizer(analyzer='word', stop_words=ita_stopwords, \n",
    "                             min_df=5, ngram_range=(1,2))\n",
    "print('count_vec')\n",
    "X_vec = count_vect.fit_transform(X_traing)\n",
    "print('tfidf')\n",
    "tf_transformer = TfidfTransformer(use_idf=True)\n",
    "X_train_tf = tf_transformer.fit_transform(X_vec)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected_features: 10 (24379, 1500)\n",
      "total features: (24379, 174028)\n"
     ]
    }
   ],
   "source": [
    "# statistical feature selection with chi2\n",
    "# rank features according to the average or the maximum Chi-squared score across all labels\n",
    "# led to most of the best classifiers while using less features. \n",
    "# fonte(http://ceur-ws.org/Vol-1094/bioasq2013_submission_8.pdf)\n",
    "Y_train = salvataggio[etichette]\n",
    "# the matrices of selected features, by category\n",
    "selected_features = [] \n",
    "chi2_model = []\n",
    "for label in etichette:\n",
    "    chi2_model.append(SelectKBest(chi2, k=1500))\n",
    "    selected_features.append(chi2_model[-1].fit_transform(X_train_tf, Y_train[label]))\n",
    "    \n",
    "    #selected_features.append(list(selector.scores_)) # featuresx10 matrix, score of each featrue for each class\n",
    "print('selected_features:',len(selected_features), selected_features[1].shape)\n",
    "print('total features:', X_train_tf.shape)\n",
    "\n",
    "# prendo dalla matrice X solo le feature che hanno un max > di una threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok 0\n",
      "ok 1\n",
      "ok 2\n",
      "ok 3\n",
      "ok 4\n",
      "ok 5\n",
      "ok 6\n",
      "ok 7\n",
      "ok 8\n",
      "ok 9\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "algo = LinearSVC(C=20)#, class_weight={0:0.2,1:1})\n",
    "models = [algo] * len(etichette)\n",
    "\n",
    "for i in range(len(etichette)):\n",
    "    models[i].fit(selected_features[i], Y_train[etichette[i]])\n",
    "    print('ok', i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['attivazione', 'canone rai', 'cessazione', 'contratto', 'credito', 'domiciliazione', 'fatturazione', 'gr-invito a chiamare - focus', 'non di competenza', 'richiamate']\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "# importo il test set per la valutazione del modello\n",
    "unito_test = []\n",
    "#label_names = []\n",
    "test_texts = list()\n",
    "test_labels = list()\n",
    "#raise SystemExit\n",
    "\n",
    "# load test data\n",
    "# carica il test set\n",
    "with codecs.open(os.path.join(DATA_FOLDER, test_db_name), 'r', errors='ignore') as file:\n",
    "    reader = csv.reader(file, delimiter=';')\n",
    "    etichette = next(reader)[3:]\n",
    "    print(etichette)\n",
    "    for row in reader:\n",
    "        # load documents\n",
    "        test_texts.append((row[1]+row[2]).replace('O:', '').replace('C:', ''))\n",
    "        # load names\n",
    "        #names_test.append(row[0])\n",
    "        # load labels\n",
    "        test_labels.append(list(map(lambda x: int(x),row[-10:])))\n",
    "        \n",
    "X_test = list(map(get_tokenizer, test_texts))\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(424, 10)\n",
      "Report:\n",
      "                              precision    recall  f1-score   support\n",
      "\n",
      "                 attivazione       0.18      1.00      0.30        73\n",
      "                  canone rai       0.31      0.92      0.46        63\n",
      "                  cessazione       0.06      0.60      0.11        10\n",
      "                   contratto       0.18      1.00      0.31        74\n",
      "                     credito       0.39      1.00      0.56       162\n",
      "              domiciliazione       0.07      1.00      0.13        21\n",
      "                fatturazione       0.30      0.99      0.46       124\n",
      "gr-invito a chiamare - focus       0.05      1.00      0.10        23\n",
      "           non di competenza       0.15      0.72      0.25        50\n",
      "                  richiamate       0.00      0.00      0.00         3\n",
      "\n",
      "                 avg / total       0.26      0.96      0.40       603\n",
      "\n",
      "\n",
      "\n",
      "accuracy 0.00235849056604\n",
      "precision micro 0.196855775803\n",
      "recall micro 0.955223880597\n",
      "f1 micro 0.326438084443\n",
      "precision macro 0.168624666236\n",
      "recall macro 0.823257040451\n",
      "f1 macro 0.267731703173\n"
     ]
    }
   ],
   "source": [
    "# apply model to data\n",
    "Y_test = pd.DataFrame(test_labels, columns=etichette)\n",
    "Y_test.head()\n",
    "\n",
    "test_doc_term = count_vect.transform(X_test)\n",
    "test_doc_idf = tf_transformer.transform(test_doc_term)\n",
    "prediction = np.zeros(Y_test.shape)\n",
    "for i in range(len(etichette)):\n",
    "    chi2_test = chi2_model[i].transform(test_doc_idf)\n",
    "    prediction[:,i] = models[i].predict(chi2_test) #np_array\n",
    "\n",
    "print(Y_test.shape)\n",
    "# print evaluation report\n",
    "print('Report:')\n",
    "print(classification_report(Y_test, prediction, target_names=etichette))\n",
    "#evaluation_report.print_evaluation(Y_test, predicted_labels)\n",
    "# conta quante etichette sono state messe\n",
    "#print(colonna, np.sum(predicted_labels))\n",
    "#print('Decision function: ')\n",
    "\n",
    "#scores = algo.decision_function(test_texts) #np_array\n",
    "\n",
    "print('\\n')\n",
    "print('accuracy', accuracy_score(Y_test, prediction))\n",
    "print('precision micro',precision_score(Y_test, prediction, average='micro') )\n",
    "print('recall micro',recall_score(Y_test, prediction, average='micro'))\n",
    "print('f1 micro', f1_score(Y_test, prediction, average='micro'))\n",
    "print('precision macro', precision_score(Y_test, prediction, average='macro') )\n",
    "print('recall macro', recall_score(Y_test, prediction, average='macro'))\n",
    "print('f1 macro', f1_score(Y_test, prediction, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# confronto gli error rate per ciascuna classe\n",
    "# predico il training set\n",
    "Y_pred_train = model.predict(texts)\n",
    "\n",
    "accuracy_test = []\n",
    "accuracy_train = []\n",
    "for i in range(predicted_labels.shape[1]):\n",
    "    accuracy_test.append(accuracy_score(Y_test[:,i], predicted_labels[:,i]))\n",
    "    accuracy_train.append(accuracy_score(Y[:,i], Y_pred_train[:,i]))\n",
    "    \n",
    "\n",
    "print ('error rate:')\n",
    "print('{:>35} {:>10}'.format('test','train'))\n",
    "for n in range(len(etichette[:10])):\n",
    "      print('{:>23}{:>12}{:>10}'.format(etichette[n], round(1-accuracy_test[n],3), round(1-accuracy_train[n],3)))\n",
    "    \n",
    "print('''\n",
    "    COME INTERPRETARE:\n",
    "    \n",
    "    Training error is low but is much lower than testing error - overfitting\n",
    "    Both errors are low - ok\n",
    "    Both errors are high - underfitting\n",
    "    Training error is high but testing is low - error in implementation or very small dataset\n",
    "''')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
